# aws cloudformation create-stack --stack-name stream-redshift --template-body file://streaming-infra.yaml --capabilities CAPABILITY_NAMED_IAM --parameters ParameterKey=S3BucketName,ParameterValue=280887266599-streaming-redshift ParameterKey=DatabaseUserPassword,ParameterValue=Ok4D6GyaELiC ParameterKey=SubnetAzA,ParameterValue=ap-southeast-1a ParameterKey=SubnetAzB,ParameterValue=ap-southeast-1b
Parameters:
  S3BucketName:
    Type: String
    Description: An Amazon S3 bucket that is used to store data, job scripts, etc.

  DatabaseUserName:
    Type: String
    Description: A user name for Redshift cluster.
    MinLength: 1
    MaxLength: 16
    AllowedPattern: '[a-zA-Z][a-zA-Z0-9]*'
    Default: dbmaster
    ConstraintDescription: must begin with a letter and contain only alphanumeric characters.

  DatabaseUserPassword:
    Type: String
    Description: A user password for Redshift cluster. (8 characters minimum, 41 characters maximum.)
    NoEcho: true
    MinLength: 8
    MaxLength: 41
    AllowedPattern: '[a-zA-Z0-9]*'
    ConstraintDescription: must contain only alphanumeric characters.

  VPCCIDR:
    Type: String
    Description: CIDR of VPC. IPv4 address range in CIDR notation.
    Default: "10.1.0.0/16"

  PublicSubnetCIDR:
    Type: String
    Description: CIDR of a public subnet A. IPv4 address range in CIDR notation.
    Default: "10.1.0.0/24"

  PrivateSubnetACIDR:
    Type: String
    Description: CIDR of a private subnet A. IPv4 address range in CIDR notation.
    Default: "10.1.100.0/24"

  PrivateSubnetBCIDR:
    Type: String
    Description: CIDR of a private subnet B. IPv4 address range in CIDR notation.
    Default: "10.1.200.0/24"

  SubnetAzA:
    Type: AWS::EC2::AvailabilityZone::Name
    Description: Availability zone for the private subnet A.

  SubnetAzB:
    Type: AWS::EC2::AvailabilityZone::Name
    Description: Availability zone for the private subnet B.

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: General
        Parameters:
          - S3BucketName
      -
        Label:
          default: Network Configuration
        Parameters:
          - VPCCIDR
          - PublicSubnetCIDR
          - PrivateSubnetACIDR
          - PrivateSubnetBCIDR
          - SubnetAzA
          - SubnetAzB

Resources:
  # S3
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref S3BucketName
      AccessControl: Private
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True

  S3CustomResource:
    DependsOn: S3Bucket
    Type: Custom::S3CustomResource
    Properties:
      ServiceToken: !GetAtt LambdaFunction.Arn
      the_bucket: !Ref S3BucketName
      the_level_three_data_file_key: data/level=3/l3_data_v3.1.csv
      the_level_one_data_file_key: data/level=1/expected_l1_data_v3.1.csv
      the_streaming_job_script_file_key: script/glue-streaming-redshift.py
      the_data_generator_script_file_key: script/kinesis-data-generator.py
      the_origin_level_three_data_url: https://raw.githubusercontent.com/NhaLeTruc/glue-streaming-redshift/main/data/l3_data_v3.1.csv
      the_origin_level_one_data_url: https://raw.githubusercontent.com/NhaLeTruc/glue-streaming-redshift/main/data/expected_l1_data_v3.1.csv
      the_origin_streaming_job_script_url: https://raw.githubusercontent.com/NhaLeTruc/glue-streaming-redshift/main/script/glue-streaming-redshift.py
      the_origin_data_generator_script_url: https://raw.githubusercontent.com/NhaLeTruc/glue-streaming-redshift/main/script/kinesis-data-generator.py

  LambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      FunctionName: !Sub 'lambda-custom-resource-${AWS::StackName}'
      Description: This is used as an AWS CloudFormation custom resource to copy job scripts from my GitHub repository to your S3 bucket.
      Handler: index.handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: 360
      Runtime: python3.8
      Code:
        ZipFile: !Sub
          - |
            import boto3
            from botocore.client import ClientError
            import cfnresponse
            import urllib.request
            def handler(event, context):
                # Init
                the_event = event['RequestType'].strip()
                print("The event is: ", the_event)
                response_data = {}
                s3 = boto3.client('s3')
                s3_resource = boto3.resource('s3')
                # Retrieve parameters
                the_bucket = event['ResourceProperties']['the_bucket'].strip()
                the_level_three_data_file_key = event['ResourceProperties']['the_level_three_data_file_key'].strip()
                the_level_one_data_file_key = event['ResourceProperties']['the_level_one_data_file_key'].strip()
                the_streaming_job_script_file_key = event['ResourceProperties']['the_streaming_job_script_file_key'].strip()
                the_data_generator_script_file_key = event['ResourceProperties']['the_data_generator_script_file_key'].strip()
                the_origin_level_three_data_url = event['ResourceProperties']['the_origin_level_three_data_url'].strip()
                the_origin_level_one_data_url = event['ResourceProperties']['the_origin_level_one_data_url'].strip()
                the_origin_streaming_job_script_url = event['ResourceProperties']['the_origin_streaming_job_script_url'].strip()
                the_origin_data_generator_script_url = event['ResourceProperties']['the_origin_data_generator_script_url'].strip()
                try:
                    if the_event in ('Create', 'Update'):
                        # Copying job script
                        try:
                            reqT = urllib.request.Request(the_origin_level_three_data_url, method='GET')
                            urlDataT = urllib.request.urlopen(reqT).read().decode('utf-8')
                            objT = s3_resource.Object(the_bucket,the_level_three_data_file_key)
                            objT.put(Body = urlDataT)

                            reqO = urllib.request.Request(the_origin_level_one_data_url, method='GET')
                            urlDataO = urllib.request.urlopen(reqO).read().decode('utf-8')
                            objO = s3_resource.Object(the_bucket,the_level_one_data_file_key)
                            objO.put(Body = urlDataO)

                            reqS = urllib.request.Request(the_origin_streaming_job_script_url, method='GET')
                            urlDataS = urllib.request.urlopen(reqS).read().decode('utf-8')
                            objS = s3_resource.Object(the_bucket,the_streaming_job_script_file_key)
                            objS.put(Body = urlDataS)

                            reqI = urllib.request.Request(the_origin_data_generator_script_url, method='GET')
                            urlDataI = urllib.request.urlopen(reqI).read().decode('utf-8')
                            objI = s3_resource.Object(the_bucket,the_data_generator_script_file_key)
                            objI.put(Body = urlDataI)

                        except ClientError as ce:
                            print("Failed to copy the source code file.")
                            print(ce)
                            print(ce.response['ResponseMetadata'])
                        except urllib.error.HTTPError as e:
                            print(e)
                    elif the_event == 'Delete':
                        try:
                          pages = []
                          paginator = s3.get_paginator('list_objects_v2')
                          for page in paginator.paginate(Bucket=the_bucket):
                            pages.extend(page['Contents'])
                          for source in pages:
                            s3.delete_object(Bucket=the_bucket,Key=source["Key"])
                        except ClientError as ce:
                            print(f"Failed to delete the files: {ce}")

                    # Everything OK... send the signal back
                    print("Completed.")
                    cfnresponse.send(event,
                                      context,
                                      cfnresponse.SUCCESS,
                                      response_data)
                except Exception as e:
                    print("Failed...")
                    print(str(e))
                    response_data['Data'] = str(e)
                    cfnresponse.send(event,
                                      context,
                                      cfnresponse.FAILED,
                                      response_data)
          - Region: !Ref AWS::Region

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub LambdaExecutionRole-${AWS::StackName}
      Description: Runs the Lambda function that has permission to upload the job scripts to the S3 bucket.
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: '2012-10-17'
      Path: "/"
      Policies:
        - PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Effect: Allow
                Resource: arn:aws:logs:*:*:*
          PolicyName: !Sub AWSLambda-CW-${AWS::StackName}
        - PolicyDocument:
            Version: '2012-10-17'
            Statement:
              -
                Effect: "Allow"
                Action:
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
              -
                Effect: "Allow"
                Action:
                  - s3:ListBucket
                  - s3:GetLifecycleConfiguration
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}
          PolicyName: !Sub AWSLambda-S3-${AWS::StackName}


  # Kinesis
  KinesisDataStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: !Sub DataStream-${AWS::StackName}
      ShardCount: 2


  # Glue
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub "database-${AWS::StackName}"

  GlueTableKinesis:
    DependsOn:
      - GlueDatabase
      - KinesisDataStream
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: !Sub "kinesis_stream_table"
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "json"
        }
        StorageDescriptor:
          Location: !Ref KinesisDataStream
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          Columns:
            - Name: seq_num
              Type: bigint
            - Name: add_order_id
              Type: bigint
            - Name: add_side
              Type: string
            - Name: add_price
              Type: double
            - Name: add_qty
              Type: bigint		
            - Name: update_order_id
              Type: bigint
            - Name: update_side
              Type: string
            - Name: update_price
              Type: double
            - Name: update_qty
              Type: bigint
            - Name: delete_order_id
              Type: bigint
            - Name: delete_side
              Type: string
            - Name: trade_order_id
              Type: bigint
            - Name: trade_side
              Type: string
            - Name: trade_price
              Type: double
            - Name: trade_qty
              Type: bigint
            - Name: time
              Type: timestamp
          SerdeInfo:
            SerializationLibrary: org.openx.data.jsonserde.JsonSerDe
          Parameters:
            typeOfData: kinesis
            streamName: !Ref KinesisDataStream
            endpointUrl: !Sub 'https://kinesis.${AWS::Region}.amazonaws.com'

  GlueL3DataTable:
    DependsOn: GlueDatabase
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: !Sub "l_three_table"
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "csv"
        }
        StorageDescriptor:
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          Columns:
          - Name: seq_num
            Type: bigint
          - Name: add_order_id
            Type: bigint
          - Name: add_side
            Type: string
          - Name: add_price
            Type: double
          - Name: add_qty
            Type: bigint		
          - Name: update_order_id
            Type: bigint
          - Name: update_side
            Type: string
          - Name: update_price
            Type: double
          - Name: update_qty
            Type: bigint
          - Name: delete_order_id
            Type: bigint
          - Name: delete_side
            Type: string
          - Name: trade_order_id
            Type: bigint
          - Name: trade_side
            Type: string
          - Name: trade_price
            Type: double
          - Name: trade_qty
            Type: bigint
          - Name: time
            Type: timestamp
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          Location: !Sub s3://${S3BucketName}/data/level=3/
          SerdeInfo:
            Parameters:
              field.delim: ","
            SerializationLibrary: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  GlueL1DataTable:
    DependsOn: GlueDatabase
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: !Sub "l_one_table"
        TableType: EXTERNAL_TABLE
        Parameters: {
          "classification": "csv"
        }
        StorageDescriptor:
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          Columns:
          - Name: time
            Type: timestamp
          - Name: bid_price
            Type: double
          - Name: ask_price
            Type: double
          - Name: bid_size
            Type: bigint
          - Name: ask_size
            Type: bigint		
          - Name: seq_num
            Type: bigint          
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          Location: !Sub s3://${S3BucketName}/data/level=1/
          SerdeInfo:
            Parameters:
              field.delim: ","
            SerializationLibrary: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  GlueServiceRole:
    DependsOn: KinesisDataStream
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
      RoleName: !Sub GlueServiceRole-${AWS::StackName}
      Path: /
      Policies:
        - PolicyDocument:
            Version: '2012-10-17'
            Statement:
              -
                Effect: "Allow"
                Action:
                  - s3:GetObject
                  - s3:PutObject
                  - s3:DeleteObject
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}/*
              -
                Effect: "Allow"
                Action:
                  - s3:ListBucket
                  - s3:GetLifecycleConfiguration
                Resource:
                  - !Sub arn:aws:s3:::${S3BucketName}
          PolicyName: !Sub GlueServiceRole-S3-${AWS::StackName}
        - PolicyDocument:
            Version: '2012-10-17'
            Statement:
              -
                Effect: "Allow"
                Action:
                  - redshift-data:ExecuteStatement
                Resource:
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:cluster:${RedshiftCluster}
              -
                Effect: "Allow"
                Action:
                  - redshift-data:DescribeStatement
                Resource:
                  - '*'
              -
                Effect: "Allow"
                Action:
                  - redshift:GetClusterCredentials
                Resource:
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbname:${RedshiftCluster}/dev
                  - !Sub arn:aws:redshift:${AWS::Region}:${AWS::AccountId}:dbuser:${RedshiftCluster}/${DatabaseUserName}
          PolicyName: !Sub GlueServiceRole-RedshiftDataAPI-${AWS::StackName}
        - PolicyDocument:
            Version: "2012-10-17"
            Statement:
              -
                Effect: "Allow"
                Action:                  
                  - kinesis:Get*
                  - kinesis:List*
                  - kinesis:Describe*
                  - kinesis:PutRecord
                  - kinesis:PutRecords
                Resource: !GetAtt KinesisDataStream.Arn
          PolicyName: !Sub GlueServiceRole-KinesisDataAPI-${AWS::StackName}
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole

  GlueStreamingJob:
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub 'streaming-kinesis2redshift${AWS::StackName}'
      Role: !Ref GlueServiceRole
      Command:
        Name: gluestreaming
        ScriptLocation: !Sub 's3://${S3BucketName}/script/glue-streaming-redshift.py'
        PythonVersion: 3
      DefaultArguments:
        --TempDir: !Sub 's3://${S3BucketName}/tmp/'
        --job-language: python
        --job-bookmark-option: job-bookmark-disable
        --enable-metrics: ''
        --enable-continuous-cloudwatch-log: true
        --additional-python-modules: boto3==1.18.13
        --src_glue_database_name: !Ref GlueDatabase
        --src_glue_table_name: !Ref GlueTableKinesis
        --dst_redshift_database_name: dev
        --dst_redshift_schema_name: book
        --deltas_redshift_table_name: deltas
        --bbo_redshift_view_name: bbo
        --dst_redshift_db_user: !Ref DatabaseUserName
        --dst_redshift_cluster_identifier: !Ref RedshiftCluster
        --redshift_connection_name: !Ref GlueRedshiftConnection
      WorkerType: G.1X
      NumberOfWorkers: 2
      GlueVersion: 3.0
      Connections:
        Connections:
          - !Ref GlueRedshiftConnection

  GlueGenerateDataJob:
    DependsOn: KinesisDataStream
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub 'glue-data-generator-${AWS::StackName}'
      Role: !Ref GlueServiceRole
      Command:
        Name: glueetl
        ScriptLocation: !Sub 's3://${S3BucketName}/script/kinesis-data-generator.py'
        PythonVersion: 3
      DefaultArguments:
        --snk_data_stream: !Sub DataStream-${AWS::StackName}
        --snk_region: !Sub ${AWS::Region}
        --src_glue_database_name: !Ref GlueDatabase
        --src_glue_table_name: !Ref GlueL3DataTable
        --messages_per_sec: 1000 # Hard Cap
        --TempDir: !Sub 's3://${S3BucketName}/tmp/'
        --job-language: python
      ExecutionProperty:
        MaxConcurrentRuns: 3
      GlueVersion: 1.0
      MaxCapacity: 0.0625   

  GlueRedshiftConnection:
    Type: AWS::Glue::Connection
    Properties:
      ConnectionInput:
        Name: !Sub 'redshift-connection-${AWS::StackName}'
        ConnectionType: JDBC
        MatchCriteria: []
        PhysicalConnectionRequirements:
          AvailabilityZone: !Ref SubnetAzA
          SecurityGroupIdList:
            - !GetAtt SecurityGroup.GroupId
          SubnetId: !Ref PrivateSubnetA
        ConnectionProperties:
          JDBC_CONNECTION_URL: !Sub "jdbc:redshift://${RedshiftCluster.Endpoint.Address}:${RedshiftCluster.Endpoint.Port}/dev"
          JDBC_ENFORCE_SSL: false
          USERNAME: !Ref DatabaseUserName
          PASSWORD: !Ref DatabaseUserPassword
      CatalogId: !Ref AWS::AccountId


  # Redshift
  RedshiftCluster:
    DependsOn: SecurityGroup
    Type: AWS::Redshift::Cluster
    Properties:
      ClusterIdentifier: !Sub "cdc-sample-${AWS::StackName}"
      ClusterType: single-node
      NodeType: dc2.large
      DBName: dev
      MasterUsername: !Ref DatabaseUserName
      MasterUserPassword: !Ref DatabaseUserPassword
      ClusterParameterGroupName: !Ref RedshiftClusterParameterGroup
      VpcSecurityGroupIds:
        - !GetAtt SecurityGroup.GroupId
      ClusterSubnetGroupName: !Ref RedshiftClusterSubnetGroup
      PubliclyAccessible: 'false'
      Port: 5439

  RedshiftClusterParameterGroup:
    Type: AWS::Redshift::ClusterParameterGroup
    Properties:
      Description: Cluster parameter group
      ParameterGroupFamily: redshift-1.0
      Parameters:
        - ParameterName: enable_user_activity_logging
          ParameterValue: 'true'

  RedshiftClusterSubnetGroup:
    Type: AWS::Redshift::ClusterSubnetGroup
    Properties:
      Description: Cluster subnet group
      SubnetIds:
        - !Ref PrivateSubnetA
        - !Ref PrivateSubnetB


  ## Networking
  VPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VPCCIDR
      EnableDnsSupport: true
      EnableDnsHostnames: true
      InstanceTenancy: default
      Tags:
        - Key: Name
          Value: !Sub "vpc-${AWS::StackName}"

  PublicSubnet:
    DependsOn: VPC
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref SubnetAzA
      CidrBlock: !Ref PublicSubnetCIDR
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "public-subnet-${AWS::StackName}"

  PrivateSubnetA:
    DependsOn: VPC
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref SubnetAzA
      CidrBlock: !Ref PrivateSubnetACIDR
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "private-subnetA-${AWS::StackName}"

  PrivateSubnetB:
    DependsOn: VPC
    Type: AWS::EC2::Subnet
    Properties:
      AvailabilityZone: !Ref SubnetAzB
      CidrBlock: !Ref PrivateSubnetBCIDR
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "private-subnetB-${AWS::StackName}"

  PublicRouteTable:
    DependsOn:
      - InternetGatewayAttachment
      - VPC
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "public-route-${AWS::StackName}"

  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref VPC
      Tags:
        - Key: Name
          Value: !Sub "private-route-${AWS::StackName}"

  PublicSubnetRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref PublicRouteTable

  PrivateSubnetARouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnetA
      RouteTableId: !Ref PrivateRouteTable

  PrivateSubnetBRouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref PrivateSubnetB
      RouteTableId: !Ref PrivateRouteTable

  SecurityGroup:
    DependsOn: VPC
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: !Sub "security-group-${AWS::StackName}"
      VpcId: !Ref VPC

  SecurityGroupIngress:
    DependsOn: SecurityGroup
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !GetAtt SecurityGroup.GroupId
      IpProtocol: tcp
      FromPort: 0
      ToPort: 65535
      SourceSecurityGroupId: !GetAtt SecurityGroup.GroupId